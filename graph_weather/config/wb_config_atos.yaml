input:
  format: zarr  # or "netcdf4"
  variables:
    training:
      basedir: /ec/res4/hpcperm/pamc/WeatherBench/zarr/training/
      filename-template: "pl_*.zarr"
      summary-stats:
        precomputed: True
        means: /ec/res4/hpcperm/syma/WeatherBench/netcdf/means-1979-2015.nc
        std-devs: /ec/res4/hpcperm/syma/WeatherBench/netcdf/sds-1979-2015.nc
    validation:
      basedir: /ec/res4/hpcperm/pamc/WeatherBench/zarr/validation/
      filename-template: "pl_*.zarr"
    test:
      basedir: /ec/res4/hpcperm/pamc/WeatherBench/zarr/test/
      filename-template: "pl_*.zarr"
    prediction:
      filename: /ec/res4/hpcperm/pamc/WeatherBench/zarr/test/pl_2020.zarr
      outname: /ec/res4/hpcperm/pamc/WeatherBench/zarr/test/pl_2020_pred1.zarr
    names:
      - z
      - t
      - q
      - w
      - u
      - v
    levels: null
  constants:
    filename: /ec/res4/hpcperm/syma/WeatherBench/netcdf/constants.nc
    names:
      - lsm
      - z_std

###################
#  OUTPUT BLOCK
###################
output:
  basedir: /ec/res4/scratch/pamc/GNN/WeatherBench
  logging:
    log-dir: /ec/res4/scratch/pamc/GNN/WeatherBench/logs
    log-interval: 25
  checkpoints:
    ckpt-dir: checkpoints
  model:
    save-top-k: 50
    checkpoint-filename: "gnn-wb-weights-{epoch:02d}-{val_wmse:.3f}"

###################
#  MODEL BLOCK
###################
model:
  debug:
    # this will detect and trace back NaNs / Infs etc. but will slow down training
    anomaly-detection: False
  wandb:
    enabled: True
  tensorboard:
    enabled: False
  dask:
    enabled: True
    temp-dir: /ec/res4/scratch/pamc/GNN/WeatherBench/dask-temp-dir
    log-dir: /ec/res4/scratch/pamc/GNN/WeatherBench/dask-log-dir
    trim-worker-memory: True
    num-workers: 8
    num-threads-per-worker: 1
    dashboard-port: 8787
    scheduler-port: 8786
  dataloader:
    num-workers:
      training: 4
      validation: 2
      inference: 4
    batch-size:
      training: 2
      inference: 2
    batch-chunk-size:
      training: 1
      inference: 4

  # miscellaneous
  precision: 16
  fast-dev-run: False
  # runs only N training batches [N = integer | null]
  # if null then we run through all the batches
  limit-batches:
    training: 5000
    validation: 500
    test: 50
    predict: 500

  # specific GNN model settings
  lead-time: 6
  max-epochs: 50
  learn-rate: 1.e-4
  hidden-dim: 64
  num-blocks: 3
  # length of the "rollout" window (see Keisler's paper)
  rollout: 4
  # Keisler's three training rounds were:
  # Round 1. ~960,000 batches @ ~0.3 seconds per batch (4-step rollout)
  # Round 2. ~90,000 batches @ ~1.0 seconds per batch (8-step rollout)
  # Round 3. ~70,000 batches @ ~1.5 seconds per batch (12-step rollout)
  # Each batch is an N-step rollout, with batch_size=1
